{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Manage Columnar Drifting on Spark Hive </center>\n",
    "author: [Vinicius R. Barros](http://github.com/muderno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/semantix/anaconda3/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: findspark in /home/semantix/anaconda3/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in /home/semantix/anaconda3/lib/python3.7/site-packages (0.25.0)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/semantix/anaconda3/lib/python3.7/site-packages (from pyspark) (0.10.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/semantix/anaconda3/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/semantix/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/semantix/anaconda3/lib/python3.7/site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/semantix/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark findspark pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jul95 = \"NASA_access_log_Jul95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# import urllib.request\n",
    "\n",
    "# url_base = \"ftp://ita.ee.lbl.gov/traces/\"\n",
    "# files = jul95\n",
    "\n",
    "# for file in files:\n",
    "#     urllib.request.urlretrieve(url_base+file+\".gz\", file+\".gz\")\n",
    "#     with gzip.open(file+\".gz\", 'rb') as f_in:\n",
    "#         with open(file, 'wb') as f_out:\n",
    "#             shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark                         # Import FindSpark\n",
    "findspark.init()                         # Initiate FindSpark \n",
    "                                         # If your $SPARK_HOME is not correct you can use: \n",
    "                                         # findspark.init('/path/to/spark/home')\n",
    "\n",
    "from pyspark import SparkContext         # Needed to create a SparkContext\n",
    "from pyspark.sql import SparkSession     # Needed to create a SparkSession\n",
    "from pyspark.sql.types import *          # Needed to define DataFrame Schema.\n",
    "from pyspark.sql.functions import *      # Needed to SQL functions as Max(), Avg(), etc...\n",
    "import pandas as pd                      # Needed to let it pretty\n",
    "import re                                # Needed for regex capability\n",
    "from datetime import datetime            # To cast dates and timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's create a Spark Context and a Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a regex pattern to break the log into columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"^(.*)\\s(.*)\\s(.*)\\s(\\[.*\\])\\s(\\\".*\\\")\\s(.*)\\s(.*)$\"\n",
    "regex = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a Schema that will be needed to transform the RDD into Dataframe. We will passa all the fields and add a logId to be an ID coloumn and pass the full line as a field for comparison. Spark SQL Types will help us to acomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema to create DataFrame with an array typed column.\n",
    "logSchema = StructType([StructField(\"logId\", IntegerType(), True),\n",
    "                       StructField(\"hostname\", StringType(), True),\n",
    "                       StructField(\"logname\", StringType(), True),\n",
    "                       StructField(\"username\", StringType(), True),\n",
    "                       StructField(\"reqTime\", TimestampType(), True),\n",
    "                       StructField(\"reqTimeStr\",StringType(), True),\n",
    "                       StructField(\"firstLine\", StringType(), True),\n",
    "                       StructField(\"finalStatus\", IntegerType(), True),\n",
    "                       StructField(\"reqSize\", IntegerType(), True),\n",
    "                       StructField(\"line\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our logic wont fit in a anonimous function, we need to difine a function to process the data. We declare `logSK` as global variable, it's a SurrogateKey to help us make the ID. In this function we try to break the string into fields and convert them to their proper SQL Types. Returning diferent objects sizes is intentional, that is easier to filter after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSK = 0\n",
    "#>>> import iso8601\n",
    "#>>> iso8601.parse_date(utcnow().isoformat())\n",
    "def splitLog(text):\n",
    "    global logSK\n",
    "    try :\n",
    "        logList     = regex.match(text).groups()\n",
    "        \n",
    "        logSK       = logSK + 1\n",
    "        hostname    = logList[0]\n",
    "        logname     = logList[1]\n",
    "        username    = logList[2]\n",
    "        reqTime     = datetime.strptime(logList[3], \"[%d/%b/%Y:%H:%M:%S %z]\")\n",
    "        reqTimeStr  = logList[3]\n",
    "        firstLine   = logList[4]\n",
    "        finalStatus = int(logList[5] if logList[5].isnumeric() else '0')\n",
    "        reqSize     = int(logList[6] if logList[6].isnumeric() else '0')\n",
    "        line        = text\n",
    "        \n",
    "        return (logSK, hostname, logname, username, reqTime, reqTimeStr, firstLine, finalStatus, reqSize, line)\n",
    "    \n",
    "    except:\n",
    "        return (logSK, \"ERROR\",\"Error while processing line \"+str(logSK), text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Spark Context we created to load the files into RDDs ([Resilient Distributed Datasets](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)). This will load the files by lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesJul = sc.textFile(jul95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And merge two RDDs into one as all the operations will be the same for the both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = linesJul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is where the magic happens. First we use map function to run trought all lines, with we use the flatMap function it will ignore lines taking all the lines together as a single block of characters. Let's cache it in memory as we'll use it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesMapped   = lines.map(lambda a: splitLog(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesSplited  = linesMapped.filter(lambda b: len(b) == 10)\n",
    "linesRejected = linesMapped.filter(lambda b: len(b) < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splited:  1891714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "      <th>_4</th>\n",
       "      <th>_5</th>\n",
       "      <th>_6</th>\n",
       "      <th>_7</th>\n",
       "      <th>_8</th>\n",
       "      <th>_9</th>\n",
       "      <th>_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:06</td>\n",
       "      <td>[01/Jul/1995:00:00:06 -0400]</td>\n",
       "      <td>\"GET /shuttle/countdown/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:09</td>\n",
       "      <td>[01/Jul/1995:00:00:09 -0400]</td>\n",
       "      <td>\"GET /shuttle/missions/sts-73/mission-sts-73.h...</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _1                    _2 _3 _4                  _5  \\\n",
       "0   1          199.72.81.55  -  - 1995-07-01 01:00:01   \n",
       "1   2  unicomp6.unicomp.net  -  - 1995-07-01 01:00:06   \n",
       "2   3        199.120.110.21  -  - 1995-07-01 01:00:09   \n",
       "\n",
       "                             _6  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]   \n",
       "1  [01/Jul/1995:00:00:06 -0400]   \n",
       "2  [01/Jul/1995:00:00:09 -0400]   \n",
       "\n",
       "                                                  _7   _8    _9  \\\n",
       "0                    \"GET /history/apollo/ HTTP/1.0\"  200  6245   \n",
       "1                 \"GET /shuttle/countdown/ HTTP/1.0\"  200  3985   \n",
       "2  \"GET /shuttle/missions/sts-73/mission-sts-73.h...  200  4085   \n",
       "\n",
       "                                                 _10  \n",
       "0  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...  \n",
       "1  unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...  \n",
       "2  199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Splited: \", linesSplited.count())\n",
    "linesSplited.toDF().limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what are the lines that couldn't be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "      <th>_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36513</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Error while processing line 36513</td>\n",
       "      <td>alyssa.p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _1     _2                                 _3        _4\n",
       "0  36513  ERROR  Error while processing line 36513  alyssa.p"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rejected: \", linesRejected.count())\n",
    "linesRejected.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a [Dataframe](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) to explore better the data. Dataframes give us the SQL functionality to explore data. \n",
    "\n",
    "As we will use this dataframe for all analysis, we can cache it to make consultations faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDF = spark.createDataFrame(linesSplited, schema= logSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make it pretty here. To not get an Out of Memory error, we can limit the dataset before converting it to Pandas dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logId</th>\n",
       "      <th>hostname</th>\n",
       "      <th>logname</th>\n",
       "      <th>username</th>\n",
       "      <th>reqTime</th>\n",
       "      <th>reqTimeStr</th>\n",
       "      <th>firstLine</th>\n",
       "      <th>finalStatus</th>\n",
       "      <th>reqSize</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:06</td>\n",
       "      <td>[01/Jul/1995:00:00:06 -0400]</td>\n",
       "      <td>\"GET /shuttle/countdown/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:09</td>\n",
       "      <td>[01/Jul/1995:00:00:09 -0400]</td>\n",
       "      <td>\"GET /shuttle/missions/sts-73/mission-sts-73.h...</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logId              hostname logname username             reqTime  \\\n",
       "0      1          199.72.81.55       -        - 1995-07-01 01:00:01   \n",
       "1      2  unicomp6.unicomp.net       -        - 1995-07-01 01:00:06   \n",
       "2      3        199.120.110.21       -        - 1995-07-01 01:00:09   \n",
       "\n",
       "                     reqTimeStr  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]   \n",
       "1  [01/Jul/1995:00:00:06 -0400]   \n",
       "2  [01/Jul/1995:00:00:09 -0400]   \n",
       "\n",
       "                                           firstLine  finalStatus  reqSize  \\\n",
       "0                    \"GET /history/apollo/ HTTP/1.0\"          200     6245   \n",
       "1                 \"GET /shuttle/countdown/ HTTP/1.0\"          200     3985   \n",
       "2  \"GET /shuttle/missions/sts-73/mission-sts-73.h...          200     4085   \n",
       "\n",
       "                                                line  \n",
       "0  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...  \n",
       "1  unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...  \n",
       "2  199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF.limit(3).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(logId,IntegerType,true),StructField(hostname,StringType,true),StructField(logname,StringType,true),StructField(username,StringType,true),StructField(reqTime,TimestampType,true),StructField(reqTimeStr,StringType,true),StructField(firstLine,StringType,true),StructField(finalStatus,IntegerType,true),StructField(reqSize,IntegerType,true),StructField(line,StringType,true)))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDF2 = logDF.withColumn(\"df2Plus1\",expr('substring_index(substring_index(firstLine, \" \", -2), \" \", 1)')).withColumn(\"df2Plus2\",expr('substring(reqTimeStr, 5, 3)')).limit(5)\n",
    "logDF1 = logDF.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDF1 = logDF1.withColumn(\"diffType\",expr('substring(reqTimeStr, 2, 2)'))\n",
    "logDF2 = logDF2.withColumn(\"diffType\",expr('substring(reqTimeStr, 2, 2)').cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDF1 = logDF1.withColumn(\"df1Plus1\",expr('substring(reqTimeStr, 2, 2)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log1: StructType(List(StructField(logId,IntegerType,true),StructField(hostname,StringType,true),StructField(logname,StringType,true),StructField(username,StringType,true),StructField(reqTime,TimestampType,true),StructField(reqTimeStr,StringType,true),StructField(firstLine,StringType,true),StructField(finalStatus,IntegerType,true),StructField(reqSize,IntegerType,true),StructField(line,StringType,true)))\n",
      "\n",
      "log2:StructType(List(StructField(logId,IntegerType,true),StructField(hostname,StringType,true),StructField(logname,StringType,true),StructField(username,StringType,true),StructField(reqTime,TimestampType,true),StructField(reqTimeStr,StringType,true),StructField(firstLine,StringType,true),StructField(finalStatus,IntegerType,true),StructField(reqSize,IntegerType,true),StructField(line,StringType,true),StructField(df2Plus1,StringType,true),StructField(df2Plus2,StringType,true),StructField(diffType,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "log1Schema = logDF.schema\n",
    "log2Schema = logDF2.schema\n",
    "print(\"log1: %s\\n\\nlog2:%s\" % (log1Schema, log2Schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(logId,IntegerType,true),\n",
       " StructField(hostname,StringType,true),\n",
       " StructField(logname,StringType,true),\n",
       " StructField(username,StringType,true),\n",
       " StructField(reqTime,TimestampType,true),\n",
       " StructField(reqTimeStr,StringType,true),\n",
       " StructField(firstLine,StringType,true),\n",
       " StructField(finalStatus,IntegerType,true),\n",
       " StructField(reqSize,IntegerType,true),\n",
       " StructField(line,StringType,true)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1Schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(logId,IntegerType,true),\n",
       " StructField(hostname,StringType,true),\n",
       " StructField(logname,StringType,true),\n",
       " StructField(username,StringType,true),\n",
       " StructField(reqTime,TimestampType,true),\n",
       " StructField(reqTimeStr,StringType,true),\n",
       " StructField(firstLine,StringType,true),\n",
       " StructField(finalStatus,IntegerType,true),\n",
       " StructField(reqSize,IntegerType,true),\n",
       " StructField(line,StringType,true),\n",
       " StructField(df2Plus1,StringType,true),\n",
       " StructField(df2Plus2,StringType,true),\n",
       " StructField(diffType,IntegerType,true)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2Schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Comprehension \n",
    "# ls1 = [column for column in logDF1.schema.fields]\n",
    "# print(\"1-> \",ls1)\n",
    "# ls2 = [column for column in logDF2.schema.fields]\n",
    "# print(\"2-> \",ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logId</th>\n",
       "      <th>hostname</th>\n",
       "      <th>logname</th>\n",
       "      <th>username</th>\n",
       "      <th>reqTime</th>\n",
       "      <th>reqTimeStr</th>\n",
       "      <th>firstLine</th>\n",
       "      <th>finalStatus</th>\n",
       "      <th>reqSize</th>\n",
       "      <th>line</th>\n",
       "      <th>diffType</th>\n",
       "      <th>df1Plus1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logId      hostname logname username             reqTime  \\\n",
       "0      1  199.72.81.55       -        - 1995-07-01 01:00:01   \n",
       "\n",
       "                     reqTimeStr                        firstLine  finalStatus  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]  \"GET /history/apollo/ HTTP/1.0\"          200   \n",
       "\n",
       "   reqSize                                               line diffType  \\\n",
       "0     6245  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...       01   \n",
       "\n",
       "  df1Plus1  \n",
       "0       01  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF1.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logId</th>\n",
       "      <th>hostname</th>\n",
       "      <th>logname</th>\n",
       "      <th>username</th>\n",
       "      <th>reqTime</th>\n",
       "      <th>reqTimeStr</th>\n",
       "      <th>firstLine</th>\n",
       "      <th>finalStatus</th>\n",
       "      <th>reqSize</th>\n",
       "      <th>line</th>\n",
       "      <th>df2Plus1</th>\n",
       "      <th>df2Plus2</th>\n",
       "      <th>diffType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logId      hostname logname username             reqTime  \\\n",
       "0      1  199.72.81.55       -        - 1995-07-01 01:00:01   \n",
       "\n",
       "                     reqTimeStr                        firstLine  finalStatus  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]  \"GET /history/apollo/ HTTP/1.0\"          200   \n",
       "\n",
       "   reqSize                                               line  \\\n",
       "0     6245  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...   \n",
       "\n",
       "           df2Plus1 df2Plus2  diffType  \n",
       "0  /history/apollo/      Jul         1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF2.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns :[StructField(df1Plus1,StringType,true)]\n",
      "\n",
      "New columns: [StructField(df2Plus1,StringType,true), StructField(df2Plus2,StringType,true)]\n",
      "\n",
      "Present columns: [StructField(logId,IntegerType,true), StructField(hostname,StringType,true), StructField(logname,StringType,true), StructField(username,StringType,true), StructField(reqTime,TimestampType,true), StructField(reqTimeStr,StringType,true), StructField(firstLine,StringType,true), StructField(finalStatus,IntegerType,true), StructField(reqSize,IntegerType,true), StructField(line,StringType,true), StructField(diffType,IntegerType,true)]\n"
     ]
    }
   ],
   "source": [
    "colNew = []\n",
    "colMissing = []\n",
    "colHas = []\n",
    "\n",
    "colMissing = [column for column in [column for column in logDF1.schema.fields] \n",
    "           if column.name not in [column.name for column in logDF2.schema.fields]]\n",
    "\n",
    "colNew = [column for column in [column for column in logDF2.schema.fields] \n",
    "          if column.name not in [column.name for column in logDF1.schema.fields]]\n",
    "\n",
    "colHas = [column for column in [column for column in logDF2.schema.fields] \n",
    "          if column.name in [column.name for column in logDF1.schema.fields]]\n",
    "print(\"Missing columns :%s\\n\\nNew columns: %s\\n\\nPresent columns: %s\" % (colMissing, colNew, colHas)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: size:11 fields ->\n",
      " [StructField(logId,IntegerType,true), StructField(hostname,StringType,true), StructField(logname,StringType,true), StructField(username,StringType,true), StructField(reqTime,TimestampType,true), StructField(reqTimeStr,StringType,true), StructField(firstLine,StringType,true), StructField(finalStatus,IntegerType,true), StructField(reqSize,IntegerType,true), StructField(line,StringType,true), StructField(diffType,IntegerType,true)]\n",
      "\n",
      "1: size:13 with colNew fields ->\n",
      " [StructField(logId,IntegerType,true), StructField(hostname,StringType,true), StructField(logname,StringType,true), StructField(username,StringType,true), StructField(reqTime,TimestampType,true), StructField(reqTimeStr,StringType,true), StructField(firstLine,StringType,true), StructField(finalStatus,IntegerType,true), StructField(reqSize,IntegerType,true), StructField(line,StringType,true), StructField(diffType,IntegerType,true), StructField(df2Plus1,StringType,true), StructField(df2Plus2,StringType,true)]\n",
      "\n",
      "2: size:14 with colNull fields ->\n",
      " [StructField(logId,IntegerType,true), StructField(hostname,StringType,true), StructField(logname,StringType,true), StructField(username,StringType,true), StructField(reqTime,TimestampType,true), StructField(reqTimeStr,StringType,true), StructField(firstLine,StringType,true), StructField(finalStatus,IntegerType,true), StructField(reqSize,IntegerType,true), StructField(line,StringType,true), StructField(diffType,IntegerType,true), StructField(df2Plus1,StringType,true), StructField(df2Plus2,StringType,true), StructField(df1Plus1,StringType,true)]\n"
     ]
    }
   ],
   "source": [
    "colAll = []\n",
    "colAll = [column for column in [column for column in logDF2.schema.fields] \n",
    "          if column.name in [column.name for column in logDF1.schema.fields]]\n",
    "print('\\n0: size:%s fields ->\\n' % len(colAll), colAll)\n",
    "colAll.extend(colNew)\n",
    "print('\\n1: size:%s with colNew fields ->\\n' % len(colAll), colAll)\n",
    "colAll.extend(colMissing)\n",
    "print('\\n2: size:%s with colNull fields ->\\n' % len(colAll), colAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(logId,IntegerType,true), StructField(hostname,StringType,true), StructField(logname,StringType,true), StructField(username,StringType,true), StructField(reqTime,TimestampType,true), StructField(reqTimeStr,StringType,true), StructField(firstLine,StringType,true), StructField(finalStatus,IntegerType,true), StructField(reqSize,IntegerType,true), StructField(line,StringType,true), StructField(diffType,IntegerType,true), StructField(df2Plus1,StringType,true), StructField(df2Plus2,StringType,true)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logId</th>\n",
       "      <th>hostname</th>\n",
       "      <th>logname</th>\n",
       "      <th>username</th>\n",
       "      <th>reqTime</th>\n",
       "      <th>reqTimeStr</th>\n",
       "      <th>firstLine</th>\n",
       "      <th>finalStatus</th>\n",
       "      <th>reqSize</th>\n",
       "      <th>line</th>\n",
       "      <th>diffType</th>\n",
       "      <th>df2Plus1</th>\n",
       "      <th>df2Plus2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "      <td>1</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:06</td>\n",
       "      <td>[01/Jul/1995:00:00:06 -0400]</td>\n",
       "      <td>\"GET /shuttle/countdown/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...</td>\n",
       "      <td>1</td>\n",
       "      <td>/shuttle/countdown/</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:09</td>\n",
       "      <td>[01/Jul/1995:00:00:09 -0400]</td>\n",
       "      <td>\"GET /shuttle/missions/sts-73/mission-sts-73.h...</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...</td>\n",
       "      <td>1</td>\n",
       "      <td>/shuttle/missions/sts-73/mission-sts-73.html</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>burger.letters.com</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:11</td>\n",
       "      <td>[01/Jul/1995:00:00:11 -0400]</td>\n",
       "      <td>\"GET /shuttle/countdown/liftoff.html HTTP/1.0\"</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>burger.letters.com - - [01/Jul/1995:00:00:11 -...</td>\n",
       "      <td>1</td>\n",
       "      <td>/shuttle/countdown/liftoff.html</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:11</td>\n",
       "      <td>[01/Jul/1995:00:00:11 -0400]</td>\n",
       "      <td>\"GET /shuttle/missions/sts-73/sts-73-patch-sma...</td>\n",
       "      <td>200</td>\n",
       "      <td>4179</td>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:11 -0400...</td>\n",
       "      <td>1</td>\n",
       "      <td>/shuttle/missions/sts-73/sts-73-patch-small.gif</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logId              hostname logname username             reqTime  \\\n",
       "0      1          199.72.81.55       -        - 1995-07-01 01:00:01   \n",
       "1      2  unicomp6.unicomp.net       -        - 1995-07-01 01:00:06   \n",
       "2      3        199.120.110.21       -        - 1995-07-01 01:00:09   \n",
       "3      4    burger.letters.com       -        - 1995-07-01 01:00:11   \n",
       "4      5        199.120.110.21       -        - 1995-07-01 01:00:11   \n",
       "\n",
       "                     reqTimeStr  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]   \n",
       "1  [01/Jul/1995:00:00:06 -0400]   \n",
       "2  [01/Jul/1995:00:00:09 -0400]   \n",
       "3  [01/Jul/1995:00:00:11 -0400]   \n",
       "4  [01/Jul/1995:00:00:11 -0400]   \n",
       "\n",
       "                                           firstLine  finalStatus  reqSize  \\\n",
       "0                    \"GET /history/apollo/ HTTP/1.0\"          200     6245   \n",
       "1                 \"GET /shuttle/countdown/ HTTP/1.0\"          200     3985   \n",
       "2  \"GET /shuttle/missions/sts-73/mission-sts-73.h...          200     4085   \n",
       "3     \"GET /shuttle/countdown/liftoff.html HTTP/1.0\"          304        0   \n",
       "4  \"GET /shuttle/missions/sts-73/sts-73-patch-sma...          200     4179   \n",
       "\n",
       "                                                line  diffType  \\\n",
       "0  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...         1   \n",
       "1  unicomp6.unicomp.net - - [01/Jul/1995:00:00:06...         1   \n",
       "2  199.120.110.21 - - [01/Jul/1995:00:00:09 -0400...         1   \n",
       "3  burger.letters.com - - [01/Jul/1995:00:00:11 -...         1   \n",
       "4  199.120.110.21 - - [01/Jul/1995:00:00:11 -0400...         1   \n",
       "\n",
       "                                          df2Plus1 df2Plus2  \n",
       "0                                 /history/apollo/      Jul  \n",
       "1                              /shuttle/countdown/      Jul  \n",
       "2     /shuttle/missions/sts-73/mission-sts-73.html      Jul  \n",
       "3                  /shuttle/countdown/liftoff.html      Jul  \n",
       "4  /shuttle/missions/sts-73/sts-73-patch-small.gif      Jul  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colSelect = []\n",
    "\n",
    "colSelect = [column for column in [column for column in logDF2.schema.fields] \n",
    "          if column.name in [column.name for column in logDF1.schema.fields]]\n",
    "colSelect.extend(colNew)\n",
    "print(colSelect)\n",
    "logDF2.select([column.name for column in colSelect]).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df2Plus1</th>\n",
       "      <th>df2Plus2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/shuttle/countdown/</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/shuttle/missions/sts-73/mission-sts-73.html</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/shuttle/countdown/liftoff.html</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/shuttle/missions/sts-73/sts-73-patch-small.gif</td>\n",
       "      <td>Jul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          df2Plus1 df2Plus2\n",
       "0                                 /history/apollo/      Jul\n",
       "1                              /shuttle/countdown/      Jul\n",
       "2     /shuttle/missions/sts-73/mission-sts-73.html      Jul\n",
       "3                  /shuttle/countdown/liftoff.html      Jul\n",
       "4  /shuttle/missions/sts-73/sts-73-patch-small.gif      Jul"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF2.select([column.name for column in colNew]).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToAlterHiveTable = [column for column in [column for column in logDF2.schema.fields] \n",
    "          if column.name not in [column.name for column in logDF1.schema.fields]]\n",
    "colsToAppendOnDataframe = [column for column in [column for column in logDF1.schema.fields] \n",
    "           if column.name not in [column.name for column in logDF2.schema.fields]]\n",
    "colsAlredyHave = logDF2.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logId</th>\n",
       "      <th>hostname</th>\n",
       "      <th>logname</th>\n",
       "      <th>username</th>\n",
       "      <th>reqTime</th>\n",
       "      <th>reqTimeStr</th>\n",
       "      <th>firstLine</th>\n",
       "      <th>finalStatus</th>\n",
       "      <th>reqSize</th>\n",
       "      <th>line</th>\n",
       "      <th>df2Plus1</th>\n",
       "      <th>df2Plus2</th>\n",
       "      <th>diffType</th>\n",
       "      <th>df1Plus1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1995-07-01 01:00:01</td>\n",
       "      <td>[01/Jul/1995:00:00:01 -0400]</td>\n",
       "      <td>\"GET /history/apollo/ HTTP/1.0\"</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logId      hostname logname username             reqTime  \\\n",
       "0      1  199.72.81.55       -        - 1995-07-01 01:00:01   \n",
       "\n",
       "                     reqTimeStr                        firstLine  finalStatus  \\\n",
       "0  [01/Jul/1995:00:00:01 -0400]  \"GET /history/apollo/ HTTP/1.0\"          200   \n",
       "\n",
       "   reqSize                                               line  \\\n",
       "0     6245  199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] ...   \n",
       "\n",
       "           df2Plus1 df2Plus2  diffType df1Plus1  \n",
       "0  /history/apollo/      Jul         1     None  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempDF = logDF2.limit(100)\n",
    "\n",
    "colsAlredyHave.extend(colsToAppendOnDataframe)\n",
    "final_struc = StructType(fields=colsAlredyHave)\n",
    "for col in colsToAppendOnDataframe:\n",
    "    tempDF = tempDF.withColumn(col.name, lit(None).cast(col.dataType))\n",
    "tempDF = tempDF.select([column.name for column in colsAlredyHave])\n",
    "rightDF = spark.createDataFrame(tempDF.rdd, schema= final_struc)\n",
    "rightDF.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Query result: ALTER TABLE datalogger ADD COLUMNS (df2Plus1 STRING, df2Plus2 STRING);\n"
     ]
    }
   ],
   "source": [
    "fieldsInfo = \", \".join([\"%s %s\" % (col.name, col.dataType.typeName().upper()) for col in colsToAlterHiveTable])\n",
    "#query = \"ALTER TABLE employee ADD COLUMNS (%s);\" % fields\n",
    "tableName=\"datalogger\"\n",
    "query = \"ALTER TABLE %(tableName)s ADD COLUMNS (%(fields)s);\" % {\"fields\":fieldsInfo, \"tableName\":tableName}\n",
    "print(\"\\n Query result:\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: 'ALTER TABLE datalogger ADD COLUMNS (df2Plus1 STRING, df2Plus2 STRING);'\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`df1Plus1`' given input columns: [diffType, reqSize, logname, reqTime, df2Plus2, hostname, finalStatus, reqTimeStr, logId, line, df2Plus1, username, firstLine];;\\n'Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, diffType#269, 'df1Plus1]\\n+- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, df2Plus2#244, cast(substring(reqTimeStr#217, 2, 2) as int) AS diffType#269]\\n   +- GlobalLimit 5\\n      +- LocalLimit 5\\n         +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, substring(reqTimeStr#217, 5, 3) AS df2Plus2#244]\\n            +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, substring_index(substring_index(firstLine#218,  , -2),  , 1) AS df2Plus1#232]\\n               +- LogicalRDD [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Vini/apache-spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Vini/apache-spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o459.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`df1Plus1`' given input columns: [diffType, reqSize, logname, reqTime, df2Plus2, hostname, finalStatus, reqTimeStr, logId, line, df2Plus1, username, firstLine];;\n'Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, diffType#269, 'df1Plus1]\n+- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, df2Plus2#244, cast(substring(reqTimeStr#217, 2, 2) as int) AS diffType#269]\n   +- GlobalLimit 5\n      +- LocalLimit 5\n         +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, substring(reqTimeStr#217, 5, 3) AS df2Plus2#244]\n            +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, substring_index(substring_index(firstLine#218,  , -2),  , 1) AS df2Plus1#232]\n               +- LogicalRDD [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a6d0f25218ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourceDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalStruc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataDriftMangerForHive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogDF2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"datalogger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-a6d0f25218ab>\u001b[0m in \u001b[0;36mdataDriftMangerForHive\u001b[0;34m(sourceDF, targetTableName, cursor)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Create a hive ordered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0msourceDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msourceDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfieldsInHivesRightOrder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Create a StructType for new Schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Vini/apache-spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Vini/apache-spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Vini/apache-spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`df1Plus1`' given input columns: [diffType, reqSize, logname, reqTime, df2Plus2, hostname, finalStatus, reqTimeStr, logId, line, df2Plus1, username, firstLine];;\\n'Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, diffType#269, 'df1Plus1]\\n+- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, df2Plus2#244, cast(substring(reqTimeStr#217, 2, 2) as int) AS diffType#269]\\n   +- GlobalLimit 5\\n      +- LocalLimit 5\\n         +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, df2Plus1#232, substring(reqTimeStr#217, 5, 3) AS df2Plus2#244]\\n            +- Project [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221, substring_index(substring_index(firstLine#218,  , -2),  , 1) AS df2Plus1#232]\\n               +- LogicalRDD [logId#212, hostname#213, logname#214, username#215, reqTime#216, reqTimeStr#217, firstLine#218, finalStatus#219, reqSize#220, line#221], false\\n\""
     ]
    }
   ],
   "source": [
    "# \n",
    "def dataDriftMangerForHive(sourceDF, targetTableName, cursor):\n",
    "    \n",
    "    # Get hive's table as dataframe using Spark SQL\n",
    "    #hiveDF = spark.sql(\"Select * from \" + targetTableName)\n",
    "    hiveDF = logDF1\n",
    "    \n",
    "    # Get the hive's table fields in right order\n",
    "    fieldsInHivesRightOrder = hiveDF.schema.fields\n",
    "    # Get the fields that sources dataframe already have\n",
    "    fieldsAlredyHave = sourceDF.schema.fields\n",
    "    \n",
    "    # Get the fields that source dataframe has the more than the hive's table dataframe.\n",
    "    fieldsToAddToHiveTable = [field for field in [field for field in fieldsAlredyHave] \n",
    "          if field.name not in [field.name for field in fieldsInHivesRightOrder]]\n",
    "    # Get the fields that hive's table dataframe has the more than the source dataframe.\n",
    "    fieldsToAppendOnDataframe = [field for field in [field for field in fieldsInHivesRightOrder] \n",
    "           if field.name not in [field.name for field in fieldsAlredyHave]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Alter the table structure if neeed.\n",
    "    if fieldsToAddToHiveTable:\n",
    "        fieldsInfo = \", \".join([\"%s %s\" % (field.name, field.dataType.typeName().upper()) for field in fieldsToAddToHiveTable])\n",
    "        query = \"ALTER TABLE %(targetTableName)s ADD COLUMNS (%(fields)s);\" % {\"fields\":fieldsInfo, \"targetTableName\":targetTableName}\n",
    "        \n",
    "        #Need  query executor using the right JDBC, not Spark's SQL JDBC\n",
    "        try:\n",
    "            #cursor.execute(query)\n",
    "            print(\"Executing: '%s'\" % query)\n",
    "        except SQLException as s:\n",
    "            return None\n",
    "        # Get the hive's table fields in new right order\n",
    "        # fieldsInHivesRightOrder = spark.sql(\"Select * from \" + targetTableName).schema.fields\n",
    "        fieldsInHivesRightOrder = hiveDF.schema.fields\n",
    "    # Append the fields that source dataframe doesn't have\n",
    "\n",
    "    for field in fieldsToAppendOnDataframe:\n",
    "        sourceDF = sourceDF.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "\n",
    "    # Create a hive ordered\n",
    "    sourceDF = sourceDF.select([field.name for field in fieldsInHivesRightOrder])\n",
    "\n",
    "    # Create a StructType for new Schema\n",
    "    finalStruc = StructType(fields=fieldsInHivesRightOrder)\n",
    "    \n",
    "    # Return a new right ordered dataframe\n",
    "    return spark.createDataFrame(sourceDF.rdd, schema = finalStruc)\n",
    "\n",
    "ret = dataDriftMangerForHive(logDF2, \"datalogger\", None)\n",
    "ret.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
